{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WieTPZAWB98r"
      },
      "source": [
        "# GCS Auth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlCkpcTnB7Ul"
      },
      "outputs": [],
      "source": [
        "print(\"Setting up GCS access...\")\n",
        "import os\n",
        "os.environ['USE_AUTH_EPHEM'] = '0'\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGdy4uJq1kSQ"
      },
      "source": [
        "# Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qE2NoOtwYdg"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output \n",
        "!pip install gcsfs\n",
        "!pip install t5==0.9.2\n",
        "!pip install -q tensorflow-text==2.8.0rc0\n",
        "clear_output()\n",
        "print(\"Installing dependencies...\")\n",
        "import functools\n",
        "import os\n",
        "import gin\n",
        "import tensorflow_gcs_config\n",
        "from google.colab import auth\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from contextlib import contextmanager\n",
        "import logging as py_logging\n",
        "import t5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2h1MRzBLtex2"
      },
      "outputs": [],
      "source": [
        "tf.app.flags.DEFINE_string ('f', '', '')\n",
        "# Set credentials for GCS reading/writing from Colab and TPU.\n",
        "TPU_TOPOLOGY = \"2x2\"\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  TPU_ADDRESS = tpu.get_master()\n",
        "  print('Running on TPU:', TPU_ADDRESS)\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "tensorflow_gcs_config.configure_gcs_from_colab_auth()\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "#LOGGING\n",
        "tf.get_logger().propagate = False\n",
        "py_logging.root.setLevel('INFO')\n",
        "\n",
        "@contextmanager\n",
        "def tf_verbosity_level(level):\n",
        "  og_level = tf.logging.get_verbosity()\n",
        "  tf.logging.set_verbosity(level)\n",
        "  yield\n",
        "  tf.logging.set_verbosity(og_level)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsUULl9wsOz6"
      },
      "source": [
        "# Variables and constants\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IUd65tS3sOcK"
      },
      "outputs": [],
      "source": [
        "representation = \"tokens\" #@param ['tokens']\n",
        "pretraining_task = \"masking\" #@param ['masking']\n",
        "scheduler = \"isr\" #@param [\"polynomial\", \"constant\", \"isr\", \"slanted\"]\n",
        "my_finetuning_task = \"single-log-injection\" #@param ['multi-log-injection', 'single-log-injection', 'classifier']\n",
        "multi_log_injection_type = \"one-to-n\" #@param [ \"one-to-n\"]\n",
        "prefix = 'SINGLE_LOG_INJECTION: ' #@param ['SINGLE_LOG_INJECTION: ', 'CLASSIFICATION: ', 'MULTI_LOG_INJECTION: ']\n",
        "TASK_NAME = \"log_injection\" #@param ['classification', 'log_injection']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0kxnD0T8EzI"
      },
      "source": [
        "# Load Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "T5h4PkSo_Kus"
      },
      "outputs": [],
      "source": [
        "vocab_model_path = f\"gs://lance2/tokenizer/{representation}/tokenizer.model\"\n",
        "vocab_path = f\"gs://lance2/tokenizer/{representation}/tokenizer.vocab\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-wq4YkN9_UkO"
      },
      "outputs": [],
      "source": [
        "from t5.data import postprocessors as t5_postprocessors\n",
        "from t5.seqio import Feature,SentencePieceVocabulary\n",
        "\n",
        "num_special_mask_tokens = 100 #@param {type: \"integer\"}\n",
        "\n",
        "def load_vocabulary():\n",
        "  return SentencePieceVocabulary(vocab_model_path, num_special_mask_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3yMW-___hYd"
      },
      "source": [
        "# Prepare Dataset for T5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glLJUm1dxIiH"
      },
      "outputs": [],
      "source": [
        "if my_finetuning_task == \"multi-log-injection\":\n",
        "  train_path = f'gs://lance2/finetune/{my_finetuning_task}/{multi_log_injection_type}/{representation}/train.tsv'\n",
        "  eval_path = f'gs://lance2/finetune/{my_finetuning_task}/{multi_log_injection_type}/{representation}/eval.tsv'\n",
        "  test_path = f'gs://lance2/finetune/{my_finetuning_task}/{multi_log_injection_type}/{representation}/test.tsv'\n",
        "else:\n",
        "  train_path = f'gs://lance2/finetune/{my_finetuning_task}/{representation}/train.tsv'\n",
        "  eval_path = f'gs://lance2/finetune/{my_finetuning_task}/{representation}/eval.tsv'\n",
        "  test_path = f'gs://lance2/finetune/{my_finetuning_task}/{representation}/test.tsv'\n",
        "\n",
        "finetune_datasets_paths = {\n",
        "    \"train\":      train_path,\n",
        "    \"validation\": eval_path,\n",
        "    # \"test\": test_path\n",
        "}\n",
        "train_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "K0NTLbyXvkCs"
      },
      "outputs": [],
      "source": [
        "def load_dataset(split, shuffle_files=True):\n",
        "  \"\"\"\n",
        "  Function to load .tsv dataset as a tf.data.Dataset in TensorFlow\n",
        "  \"\"\"\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "\n",
        "  ds = tf.data.TextLineDataset(finetune_datasets_paths[split])\n",
        "  ds = ds.map(functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False)\n",
        "                          , \n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n",
        "  return ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J4_UFsVmSPk"
      },
      "source": [
        "### A few examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__RNWuimAxS9"
      },
      "outputs": [],
      "source": [
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(load_dataset(\"validation\").take(5)):\n",
        "  print(ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsbHi89ZA5-j"
      },
      "source": [
        "# Dataset Prepocessing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4bJZPQgjxKZ1"
      },
      "outputs": [],
      "source": [
        "from tensorflow_datasets.core.utils.type_utils import Shape\n",
        "def preprocessing(ds):\n",
        "  \"\"\"\n",
        "  Preprocess function to convert the tf.data.Dataset into a text-to-text format,\n",
        "  with both inputs and targets fields.\n",
        "  Param: tf.data.Dataset\n",
        "  Return: text-to-text format\n",
        "  \"\"\"\n",
        "  def to_inputs_and_targets(ex):\n",
        "    x_input = tf.strings.strip(prefix + ex['input'])\n",
        "    y_label = tf.strings.strip(ex['output']) \n",
        "    inputs = tf.strings.join([x_input], separator=' ')\n",
        "    class_label = tf.strings.join([y_label], separator=' ')\n",
        "    return {'inputs': inputs, 'targets': class_label}\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vymEYLbQCBRY"
      },
      "source": [
        "### A few examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOyas1ZqBVgE"
      },
      "outputs": [],
      "source": [
        "print(\"A few preprocessed train examples...\")\n",
        "sample = tfds.as_numpy(preprocessing(load_dataset(\"train\").take(5)))\n",
        "for ex in sample:\n",
        "  print(ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OkQeLeh8Rst"
      },
      "source": [
        "# Creating Task and Mixture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PobLvzL18zzR"
      },
      "outputs": [],
      "source": [
        "DEFAULT_OUTPUT_FEATURES = {\n",
        "    \"inputs\": Feature(\n",
        "        vocabulary=load_vocabulary(), add_eos=True, required=False),\n",
        "    \"targets\": Feature(\n",
        "        vocabulary=load_vocabulary(), add_eos=True)\n",
        "    }\n",
        "\n",
        "# TASK\n",
        "t5.data.TaskRegistry.remove(TASK_NAME)\n",
        "t5.data.TaskRegistry.add(\n",
        "    TASK_NAME,\n",
        "    # Function which returns a tf.data.Dataset\n",
        "    dataset_fn=load_dataset,\n",
        "    splits=[\"train\",\n",
        "            \"validation\",\n",
        "            # \"test\"\n",
        "            ],\n",
        "    # List of functions that preprocess the input tf.data.Dataset\n",
        "    text_preprocessor=[preprocessing],\n",
        "    # Accuracy is used as evaluation metric\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        "    # Not required, helps for mixing and auto-caching\n",
        "    # num_input_examples=num_input_examples,\n",
        "    output_features = DEFAULT_OUTPUT_FEATURES\n",
        ")\n",
        "\n",
        "MIXTURE_NAME = \"finetuning\" #@param{ type : \"string\"}\n",
        "\n",
        "# MIXTURE\n",
        "t5.data.MixtureRegistry.remove(MIXTURE_NAME)\n",
        "t5.data.MixtureRegistry.add(\n",
        "    MIXTURE_NAME,\n",
        "    # List of tasks\n",
        "    [TASK_NAME],\n",
        "    default_rate=1.0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwqLrVdGB6yy"
      },
      "source": [
        "### A few examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVHcIrjkA93r"
      },
      "outputs": [],
      "source": [
        "finetuning_task = t5.data.TaskRegistry.get(TASK_NAME)\n",
        "ds = finetuning_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 512, \"targets\": 512})\n",
        "print(\"A few preprocessed training examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(5)):\n",
        "  print(ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQobwjpVCJbu"
      },
      "source": [
        "# Creating Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GKKGuWpXCMNV"
      },
      "outputs": [],
      "source": [
        "# Storage paths\n",
        "if my_finetuning_task == \"multi-log-injection\":\n",
        "  FINETUNE_MODEL_DIR = f\"gs://lance2/finetuned-model/{my_finetuning_task}/{multi_log_injection_type}/{representation}/{scheduler}\"\n",
        "else:\n",
        "  FINETUNE_MODEL_DIR = f\"gs://lance2/finetuned-model/{my_finetuning_task}/{representation}/{scheduler}\"\n",
        "  \n",
        "PRETRAIN_MODEL_DIR= f'gs://lance2/pretrained-model/{representation}/{pretraining_task}/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QmaDaY39DwxR"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "\n",
        "# Learning rate properties\n",
        "starter_learning_rate = 0.01 #@param {type : \"number\"}\n",
        "end_learning_rate = 0.001 #@param {type : \"number\"}\n",
        "decay_steps = 10000 #@param {type : \"integer\"}\n",
        "\n",
        "learning_rate_fn = PolynomialDecay(\n",
        "     starter_learning_rate,\n",
        "     decay_steps,\n",
        "     end_learning_rate,\n",
        "     power=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3Qx699vN302"
      },
      "outputs": [],
      "source": [
        "from mesh_tensorflow.transformer.learning_rate_schedules import slanted_triangular, truncated_rsqrt\n",
        "from t5 import models\n",
        "\n",
        "# Learning rate schedule fn\n",
        "if scheduler == 'polynomial':\n",
        "  learning_rate_scheduler = learning_rate_fn\n",
        "elif scheduler == 'isr':\n",
        "  learning_rate_scheduler = truncated_rsqrt\n",
        "elif scheduler == 'slanted':\n",
        "  learning_rate_scheduler = slanted_triangular\n",
        "else: \n",
        "  learning_rate_scheduler = 0.001\n",
        "\n",
        "print(learning_rate_scheduler)\n",
        "\n",
        "# Model properties\n",
        "MODEL_SIZE = \"small\" \n",
        "model_parallelism, train_batch_size, keep_checkpoint_max = {\n",
        "    \"small\": (1, 128, 100),\n",
        "    \"base\": (2, 128, 8),\n",
        "    \"large\": (8, 64, 4),\n",
        "    \"3B\": (8, 16, 1),\n",
        "    \"11B\": (8, 16, 1)}[MODEL_SIZE]\n",
        "\n",
        "\n",
        "model = t5.models.MtfModel(\n",
        "    model_dir=FINETUNE_MODEL_DIR,\n",
        "    tpu=TPU_ADDRESS,\n",
        "    model_parallelism=model_parallelism,\n",
        "    batch_size=train_batch_size, \n",
        "    sequence_length={\"inputs\": 1024, \"targets\": 1024},\n",
        "    learning_rate_schedule = learning_rate_scheduler, \n",
        "    save_checkpoints_steps=10000,\n",
        "    keep_checkpoint_max=keep_checkpoint_max\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR7ZLccg9cwS"
      },
      "source": [
        "# Learning Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qLQ5I9_9hDR"
      },
      "outputs": [],
      "source": [
        "BASE_DIR = f\"gs://lance2/learning_rate_scheduler/{scheduler}\"\n",
        "print(BASE_DIR)\n",
        "GIN_FILENAME = \"operative_config.gin\" #@param {type : \"string\"}\n",
        "# 'no_pretraining_operative_config.gin'\n",
        "remote_gin_path = os.path.join(BASE_DIR, GIN_FILENAME)\n",
        "LOCAL_GIN_PATH = f\"/content/{GIN_FILENAME}\"\n",
        "!gsutil -m cp $remote_gin_path $LOCAL_GIN_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5puQKZ4EuQ1"
      },
      "outputs": [],
      "source": [
        "with gin.unlock_config():\n",
        "    gin.parse_config_file(LOCAL_GIN_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW5LjJWKq2Op"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6oHp5ScE7nf2"
      },
      "outputs": [],
      "source": [
        "TRAIN_STEPS =  100000 #@param {type: \"integer\"}\n",
        "\n",
        "# Stat finetuning \n",
        "model.finetune(mixture_or_task_name=MIXTURE_NAME,\n",
        "               finetune_steps=TRAIN_STEPS,\n",
        "               pretrained_model_dir=PRETRAIN_MODEL_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4nme9yl-kL9"
      },
      "source": [
        "# Evaluating the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BF4VPrv4kUo0"
      },
      "outputs": [],
      "source": [
        "checkpoints = []\n",
        "for check in range(500000, 600000, 10000):\n",
        "  checkpoints.append(check)\n",
        "# checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NAo_rui4HXcT"
      },
      "outputs": [],
      "source": [
        "from t5 import models\n",
        "\n",
        "model = t5.models.MtfModel(FINETUNE_MODEL_DIR, tpu=TPU_ADDRESS)\n",
        "# Evaluate on the validation sets of the tasks in our mixture\n",
        "model.bach_size=16\n",
        "model.eval(\n",
        "    mixture_or_task_name=MIXTURE_NAME,\n",
        "    checkpoint_steps=checkpoints,\n",
        "    # checkpoint_steps=[600000],\n",
        "    # split='test'\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Finetuning-new.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}