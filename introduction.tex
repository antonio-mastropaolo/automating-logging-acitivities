% !TEX root = main.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction} \label{sec:intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{table/comparison.tex}

The practice of injecting log statements in applications' code is widely adopted both in industry and open source projects \cite{oliner2012advances}. Indeed, log statements are instrumental to support several software-related activities, including program comprehension and debugging \cite{lu2017log,gurumdimma2016crude}. Given its popularity, it comes without surprise the proliferation of libraries to support logging activities: just for Java some possible options are Log4j \cite{log4j}, JCL \cite{jcl}, slf4j \cite{slf4j}, and logback \cite{logback}.

While logging it is usually perceived as a good practice, it comes with its own drawbacks: Excessive logging could negatively impact performance and, if not carefully conceived, log statements can result in security issues such as providing access to user credentials or sensitive information. 

Also, researchers documented several bad practices that should be avoided while logging code \cite{Chen:icse2017,Li:icse2019}. 
In general, logging poses several challenges to software developers. First, they need to decide \emph{what to log}, by finding the right amount of log statements needed in the application without, however, flood it with useless log statements. Second, developers must \emph{log at the proper level}, namely select the proper log level for each entry (\eg info, warning, error). Third, log statements must be accompanied by \emph{meaningful and informative} log messages that can be easily understood. 

To support developers in these activities, researchers proposed techniques and tools automating specific aspects of logging, such as recommending (i) where/what to log \cite{yuan2010sherlog,jia2018smartlog,li2018studying,li2020shall}, and (ii) the right level to use for a given log statement \cite{yuan2012characterizing,oliner2012advances,li2017log,li2020qualitative,li2021deeplv}. Recently, Mastropaolo \etal \cite{mastropaolo2022using} presented LANCE, an approach built on top of a Text-To-Text-Transfer-Transformer (T5) deep learning (DL) model \cite{raffel2019exploring} trained to generate and inject a complete log statement in a Java method provided as input. The T5 model has been pre-trained on a set of $\sim$6.8M Java methods using the classic ``masked language modeling'' objective \cite{raffel2019exploring}. In the case of LANCE, this means that during pre-training the model is provided as input a Java method with 15\% of its tokens masked and it is expected to predict the masked tokens. Such a pre-training task provides T5 with knowledge about the language of interest (\ie Java). Once pre-trained, the model has been fine-tuned for the specific task of interest. In this case, the authors selected $\sim$62k Java methods and removed from them exactly one log statement asking the model to generate and inject it, thus deciding \emph{where} to log (\ie in which part of the method), which \emph{log level} to use, and \emph{what} to log (\ie generate a meaningful log message in natural language). LANCE is the first approach supporting developers in all these activities. The presented empirical evaluation showed that LANCE was able to correctly predict the appropriate location of a log statement and its level in $\sim$66\% of cases, while the approach struggled in predicting a meaningful log message, being successful in 15.2\% of test instances. While LANCE represents a substantial step ahead in logging automation, it comes with some limitations. First, it assumes that only one log statement is needed in a Java method provided as input. This is due to the training procedure employed by the authors that asks the model to always generate a single log statement. While such an assumption simplifies the addressed problem, it is not empirically supported, since \textcolor{red}{XX}\% of Java methods in our dataset makes use of more than one log statement. Second, given a Java method, LANCE cannot even assess whether log statements are needed at all. 

\eject

Indeed, in some cases, enough log statements may be already present in the method or, maybe, the method does not feature statements that would benefit from logging. Finally,  while LANCE achieves good performance in predicting the log statement location and level, it showed substantial limitations in synthesizing meaningful natural language log messages. In this work we study how to partially address these limitations.

We start replicating LANCE by training and testing it on a dataset 3.6 times larger than the one used by Mastropaolo \etal \cite{mastropaolo2022using} (230k training instances \emph{vs} 63k). Besides being larger, our dataset, as we will explain, features a more variegate set of log statements. Then, we present \approach as an extension of LANCE able to (i) discriminate between methods \emph{needing} and \emph{not needing} the injection of new log statements; and (ii) in case a need for log statements is identified, \approach, differently from LANCE, can decide the proper number of log statements to inject (which can be higher than one) and properly place them in the correct position. We found that \approach can correctly predict the need for log statements with an accuracy higher than 90\%. Also, when log statements are needed, it can generate and inject in the right position multiple complete log statements in $\sim$23\% of cases, against the $\sim$27\% success rate achieved in the simpler scenario of single-log injection (\ie the same problem addressed by LANCE).   

Finally, in \approach we attempted to improve the performance achieved in the generation of meaningful log messages by exploiting a combination of DL and Information Retrieval (IR). Indeed, based on the results reported in \cite{mastropaolo2022using}, the generation of log messages really looked like the Achilles' heel of DL-based log generation. However, we found that by simply increasing the size of the training dataset, the ability of LANCE in predicting meaningful log messages is boosted to 30\% (+100\% as compared to what reported in \cite{mastropaolo2022using}). Starting from this level of performance, the combination of DL and IR we propose in \approach only provides a further +5\% relative improvement (31.55\%) in log message generation, pointing to the need for additional research on this problem.

\tabref{tab:sota} shows how \approach widens the support provided to developers in the automation of logging activities. Indeed, it is the only one deciding whether log statements are needed in a method and, in case of positive answer, synthesizing multiple and complete log statements, and inject them in the correct position. 

%Increasing the complexity of the problem is likely to result in a decrease of accuracy. However, \approach exploits a new fine-tuning strategy combining DL and Information Retrieval (IR) to boost the prediction accuracy when it comes to the most challenging sub-problem involved in log injection: the generation of a meaningful log message. To explain our fine-tuning training procedure let us indicate with $<$$m_i$, $LS=\{ls_1, ls_2, \dots, ls_n\}$$>$ a training instance in our dataset, with $m_i$ being a Java method from which we removed $n$ log statements and $LS$ being the set of removed statements the model is expected to generate. We identify in the training set the top-$k$ methods being most similar to $m_i$ and extract the set of log messages $M$ used in them. We use $M$ to augment our training instance, that becomes a triplet composed by $<$$m_i$, $LS=\{ls_1, ls_2, \dots, ls_n\}$, $M$$>$. The idea is that T5 can learn how to write a meaningful log message for $LS$ by looking at examples of log messages ($M$) used in methods similar to the one it is provided as input ($m_i$). While testing our approach on a previously unseen method $m_j$, we follow the same procedure by identifying the top-$k$ most similar methods in our training set.