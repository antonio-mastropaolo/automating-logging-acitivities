% !TEX root = main.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Study Design} \label{sec:design}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The \emph{goal} of our study is to evaluate the performance of \approach in supporting logging activities in \java methods. In particular, we focus on three scenarios: single log injection, in which we compare with the state-of-the-art approach LANCE \cite{mastropaolo2022using}; multi-log injection; and deciding wether log statements are needed or not in a given \java method. The context is represented by the test datasets reported in \tabref{tab:ds-summary-1} (for single and multi-log injection) and \tabref{tab:ds-summary-2} (for deciding whether logging is needed or not).

We aim at answering the following research questions:

\begin{itemize}

\item[\textbf{RQ$_1$:}]\textit{To what extent is \approach able to correctly inject a single complete logging statement in Java methods?} RQ$_1$ mirrors the study performed by Mastropaolo \etal~\cite{mastropaolo2022using} when presenting LANCE. In particular, we experiment \approach in the same scenario presented in \cite{mastropaolo2022using}: The injection of a single log statement in a given Java method. We compare the performance of \approach with that on LANCE when training and testing them on the same dataset. 

\item[\textbf{RQ$_2$:}]\textit{To what extent is \approach able to correctly inject multiple log statements when needed?} RQ$_2$ tests our approach in the most challenging scenario of generating and injecting from 1 to $n$ log statements in a \java method as needed.

\item[\textbf{RQ$_3$:}]\textit{To what extent is \approach able to properly decide when to inject log statements?} RQ$_3$ analyzes the accuracy of \approach in predicting whether or not log statements are needed in a given \java method, a problem that was oversaw in the work presenting LANCE \cite{mastropaolo2022using}.

\end{itemize}

\subsection{Data Collection and Analysis}

To answer RQ$_1$ we run both \approach and LANCE against the test set described in \tabref{tab:ds-summary-1} for the single log generation task. The only difference is that LANCE has been trained on the dataset not featuring the exemplar log messages added through IR (\ie the row \emph{Fine-tuning: Single Log Generation} in \tabref{tab:ds-summary-1}), while \approach exploits this information (row \emph{Fine-tuning: Single Log Generation with IR} in \tabref{tab:ds-summary-1}). However, the training and test instances are exactly the same, allowing for a direct comparison.

We assess the performance of the two techniques using the same evaluation schema of Mastropaolo \etal~\cite{mastropaolo2022using}. In particular, we contrast the predictions generated by the two models against the expected output (\ie the \java method provided as input with the addition of the correct log statement). Note that generating and injecting a log statement  (\eg \texttt{LoggerUtil.debug("execution ok")}) involves correctly predicting several information: (i) the name of the variable used for the logging (\ie \texttt{LoggerUtil}); (ii) the log level (\ie \texttt{debug}); (iii) the log message (\ie \texttt{"execution ok"}); and (iv) the position in the method in which the log statement must be injected. Thus, when a prediction is generated by the experimented models, three scenarios are possible:

\begin{description}
	\item[Correct prediction:] A prediction that correctly captures all above-described information, \ie it matches the name used for the variable, the log level, message, and position as written by the original developers.

	\item[Partially correct prediction:] A prediction that correctly captures a subset of the needed information (\eg it correctly generates the log statement but it injects it in the wrong position).

	\item[Wrong prediction:] None of the above-described information is correctly predicted.
\end{description}

As shown by Mastropaolo \etal~\cite{mastropaolo2022using}, the prediction of the log message is the most challenging part since, to correctly predict it, it requires the model to synthesize a message using the same terminology adopted by the original developers. Also, automatically assessing the correctness of the prediction for what concerns the log message is quite limitative considering that a message generated by a model using a different terminology as compared to the target log statement but being semantically equivalent to it is considered wrong. For this reason, we answer RQ$_1$ through the following combination of quantitative and qualitative analysis. On the quantitative side, we report for both \approach and LANCE the percentage of correct, partially correct, and wrong predictions. For the partially correct, we report the percentage of cases in which each of the ``log statement components'' (\ie variable name, log level, log message, and log position) has been correctly predicted. As for the percentage of correct and partially correct predictions, we pairwise compare them among the experimented techniques, using the McNemar's test \cite{mcnemar}, which is a proportion test suitable to pairwise compare dichotomous results of two different treatments. We complement the McNemar's test with the Odds Ratio (OR) effect size. We use the Holm's correction procedure \cite{Holm1979a} to account for multiple comparisons. Finally, we also compute the complementarity between LANCE and LEONID. 
To this extent, we report the percentage of perfect predictions achieved by LANCE and LEONID that are in common, that only LANCE was able to generate and to close the circle, the perfect predictions that only LEONID was capable of synthesizing.



Concerning the quality of the log messages generated by the two techniques, looking for exact matches (\ie cases in which the generated log message is identical to the one written by developers) is quite limitative considering that a prediction including a message different but semantically equivalent to the target one could still be valuable. For this reason, we also compute the following four metrics used in Natural Language Processing (NLP) for the assessment of automatically generated text:

\textbf{BLEU}~\cite{papineni2002bleu} assesses the quality of the automatically generated text by comparing it to the target text. The BLEU score ranges between 0 (the sequences are completely different) and 1 (the sequences are identical) and can be computed considering four different sizes $n$. In detail, for given a size $n$, the candidate (\ie automatically generated text) and target texts are broken into \textit{n}-grams and the algorithm determines how many \textit{n}-grams of the candidate text overlap in the target text. We report different  BLEU-\{1, 2, 3, 4\} and their geometric mean (\ie BLEU-A).

\textbf{METEOR}~\cite{meteor} is a metric based on the harmonic mean of unigram precision and recall. Compared to BLEU, METEOR uses stemming and synonyms matching to better match the human perception of sentences with similar meanings. Also in this case values range from 0 to 1, with 1 being a perfect match.

\textbf{ROUGE}~\cite{lin2004rouge} is a set of metrics focusing on automatic summarization tasks. We use the ROUGE-LCS (Longest Common Subsequence) variant which returns three values: the recall computed as \textit{LCS(X,Y)/length(X)}, the precision computed as \textit{LCS(X,Y)/length(Y)}, and the F-measure computed as the harmonic mean of recall and precision where \textit{X} and \textit{Y} represent two sequences of tokens.

\textbf{LEVENSHTEIN Distance}~\cite{levenshtein1966} provides an indication of the percentage of words that must be changed in the synthesized log message to match the target log message. This is accomplished by computing the normalized token-level Levenshtein distance \cite{levenshtein1966} (NTLev) between the predicted log message ($LM_p$) and the target log message ($LM_t$).

% Un-comment if we have space
%$$
%NTLev(LM_p, LM_t) = \frac{\mathit{TLev}(LM_p, LM_t)}{\max({\{}|LM_p|, |LM_t|\})}
%$$
%
%\noindent with $\mathit{TLev}$ representing the token-level Levenshtein distance between the two log messages. 

Such a metric can act as a proxy to estimate the effort requested to a developer in editing a non-perfect log message suggested by the model into the target one.

We also statistically compare the distribution of the BLEU-4 (computed at sentence level), METEOR, ROUGE, and LEVENSHTEIN distance related to the predictions generated by \approach and LANCE. We assume a significance level of 95\% and use the Wilcoxon signed-rank test \cite{wilcoxon}, adjusting $p$-values to account for multiple comparisons using the Holm's correction procedure \cite{Holm1979a}. The  Cliff's Delta ($d$) is used as effect size measure \cite{Gris2005a}. We follow well-established guidelines to interpret the effect size: negligible for $|d| < 0.10$, small for $0.10 \le |d| < 0.33$, medium for $0.33 \le |d| < 0.474$, and large for $|d| \ge 0.474$ \cite{Gris2005a}.

On the qualitative side, we manually inspected 300 of the partially correct predictions generated by both techniques and having all information but the log message correctly predicted. The goal of the inspection was to verify whether the generated log message, while different from the target one, was semantically equivalent to it. To this aim, two of the authors independently inspected all 600 log messages (300 for each approach), with $\sim$11\% (70) arisen conflicts being solved by a third author. We report the percentage of ``wrong'' log messages generated by both techniques classified as semantically equivalent.

To answer RQ$_2$ and evaluate the extent to which \approach is able to correctly inject multiple log statements, we run \approach against the test set reported in \tabref{tab:ds-summary-1} (see row \emph{Fine-tuning: Multi-log Injection with IR}). We then report the percentage of correct predictions generated by the approach (\ie methods for which all $n$ log statements that \approach was supposed to generate and inject have been correctly predicted). In this case we do not compute the partially correct predictions since, if a prediction is not completely correct, it is not possible to match the generated log statements with the target ones to compare them. To make this concept more clear, let us assume the case in which \approach was supposed to generate two log statements $s_1$ and $s_2$ but it only injects one statement $s_i$, being different from both $s_1$ and $s_2$. We cannot know whether $s_i$ should be compared with $s_1$ or with $s_2$ to assess the percentage of partially correct predictions in terms of \eg log level. For this reason, we only focus on the predictions being 100\% correct (\ie the method generated in output is identical to the target one). We complement this quantitative analysis with qualitative examples of predictions.

To answer RQ$_3$, we run \approach against the four test sets presented in \tabref{tab:ds-summary-2}, reporting the confusion matrix of the generated predictions and the corresponding values for accuracy, recall, and precision. We compare these results with those of two constant classifiers and a random classifier. The \emph{optimistic} constant classifier always predicts \emph{true} (\ie the method is in need of log statements), while the \emph{pessimistic} one always predicts \emph{false} (\ie the method does not need log statements). The random classifier just randomly predicts \emph{true} or \emph{false} for each input instance. \ANTONIO{ We use the same statistical analysis described for RQ$_1$ to compare \approach with the baselines. Make sure here if we have enough space to fit the table, eventually}