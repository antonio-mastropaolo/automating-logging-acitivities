% !TEX root = main.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Study Design} \label{sec:design}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The \emph{goal} of our study is to evaluate the performance of \approach in supporting logging activities in \java methods. We focus on three scenarios: single log injection, in which we compare with the state-of-the-art approach LANCE \cite{mastropaolo2022using}; multi-log injection; and deciding wether log statements are needed or not in a given \java method. The context is represented by the test datasets reported in \tabref{tab:ds-summary-1} (single and multi-log injection) and \tabref{tab:ds-summary-2} (deciding whether logging is needed).

We aim at answering the following research questions:

\begin{itemize}[itemindent=0.3cm]

\item[\textbf{RQ$_1$:}]\textit{To what extent is \approach able to correctly inject a single complete logging statement in Java methods?} RQ$_1$ mirrors the study performed by Mastropaolo \etal~\cite{mastropaolo2022using} when presenting LANCE. We experiment \approach in the same scenario presented in \cite{mastropaolo2022using}: The injection of a single log statement in a given Java method. We compare the performance of \approach with that of LANCE when training and testing them on the same dataset. 

\item[\textbf{RQ$_2$:}]\textit{To what extent is \approach able to correctly inject multiple log statements when needed?} RQ$_2$ tests \approach in the more challenging scenario of injecting from 1 to $n$ log statements in a \java method, as needed.

\item[\textbf{RQ$_3$:}]\textit{To what extent is \approach able to properly decide when to inject log statements?} RQ$_3$ analyzes the accuracy of \approach in predicting whether or not log statements are needed in a given \java method, a problem that was oversaw in the work presenting LANCE \cite{mastropaolo2022using}.

\end{itemize}

\subsection{Data Collection and Analysis}

To answer RQ$_1$ we run both \approach and LANCE against the test set described in \tabref{tab:ds-summary-1} for the single log generation task. The only difference is that LANCE has been trained on the dataset not featuring the exemplar log messages added through IR (row \emph{Fine-tuning: Single Log Generation} in \tabref{tab:ds-summary-1}), while \approach exploits this information (row \emph{Fine-tuning: Single Log Generation with IR} in \tabref{tab:ds-summary-1}). However, the training and test instances are exactly the same, allowing for a direct comparison. We assess the performance of the two techniques using the same evaluation schema of Mastropaolo \etal~\cite{mastropaolo2022using}. In particular, we contrast the predictions generated by the two models against the expected output (\ie the \java method provided as input with the addition of the correct log statement). Note that generating and injecting a log statement  (\eg \texttt{LoggerUtil.debug("execution ok")}) involves correctly predicting several information: (i) the name of the variable used for the logging (\ie \texttt{LoggerUtil}); (ii) the log level (\ie \texttt{debug}); (iii) the log message (\ie \texttt{"execution ok"}); and (iv) the position in the method in which the log statement must be injected. Thus, when a prediction is generated, three scenarios are possible:

\textbf{Correct prediction:} A prediction that correctly captures all above-described information, \ie it matches the name used for the variable, the log level, message, and position as written by the original developers.

\textbf{Partially correct prediction:} A prediction that correctly captures a subset of the needed information (\eg it correctly generates the log statement but injects it in the wrong position).

\textbf{Wrong prediction:} None of the above-described information is correctly predicted.

We answer RQ$_1$ through the following combination of quantitative and qualitative analysis. On the quantitative side, we report for both \approach and LANCE the percentage of correct, partially correct, and wrong predictions. For the partially correct, we report the percentage of cases in which each of the ``log statement components'' (\ie variable name, log level, log message, and log position) has been correctly predicted. As for the percentage of correct and partially correct predictions, we pairwise compare them among the experimented techniques, using the McNemar's test \cite{mcnemar}, which is a proportion test suitable to pairwise compare dichotomous results of two different treatments. We complement the McNemar's test with the Odds Ratio (OR) effect size. We use the Holm's correction procedure \cite{Holm1979a} to account for multiple comparisons.

Concerning the quality of the log messages generated by the two techniques, looking for exact matches (\ie cases in which the generated log message is identical to the one written by developers) is quite limitative considering that a prediction including a message different but semantically equivalent to the target one could still be valuable. For this reason, we also compute the following four metrics used in Natural Language Processing (NLP) for the assessment of automatically generated text:

\textbf{BLEU}~\cite{papineni2002bleu} assesses the quality of the automatically generated text in terms of $n$-grams overlap with respect to the target text. The BLEU score ranges between 0 (the sequences are completely different) and 1 (the sequences are identical) and can be computed considering four different values of $n$ (\ie BLEU-\{1, 2, 3, 4\}). Besides these four variants, we also compute their geometric mean (\ie BLEU-A).

\textbf{METEOR}~\cite{meteor} is a metric based on the harmonic mean of unigram precision and recall. Compared to BLEU, METEOR uses stemming and synonyms matching to better reflect the human perception of sentences with similar meanings. Values range from 0 to 1, with 1 being a perfect match.

\textbf{ROUGE}~\cite{lin2004rouge} is a set of metrics focusing on automatic summarization tasks. We use the ROUGE-LCS (Longest Common Subsequence) variant which returns three values: the recall computed as \textit{LCS(X,Y)/length(X)}, the precision computed as \textit{LCS(X,Y)/length(Y)}, and the F-measure computed as the harmonic mean of recall and precision, where \textit{X} and \textit{Y} represent two sequences of tokens.

\textbf{LEVENSHTEIN Distance}~\cite{levenshtein1966} provides an indication of the percentage of words that must be changed in the synthesized log message to match the target log message. This is accomplished by computing the normalized token-level Levenshtein distance \cite{levenshtein1966} (NTLev) between the predicted log message and the target one. Such a metric can act as a proxy to estimate the effort required to a developer in fixing a non-perfect log message suggested by the model.

% Un-comment if we have space
%$$
%NTLev(LM_p, LM_t) = \frac{\mathit{TLev}(LM_p, LM_t)}{\max({\{}|LM_p|, |LM_t|\})}
%$$
%
%\noindent with $\mathit{TLev}$ representing the token-level Levenshtein distance between the two log messages. 



We also statistically compare the distribution of the BLEU-4 (computed at sentence level), METEOR, ROUGE, and LEVENSHTEIN distance related to the predictions generated by \approach and LANCE. We assume a significance level of 95\% and use the Wilcoxon signed-rank test \cite{wilcoxon}, adjusting $p$-values using the Holm's correction \cite{Holm1979a}. The  Cliff's Delta ($d$) is used as effect size \cite{Gris2005a} and it is considered: negligible for $|d| < 0.10$, small for $0.10 \le |d| < 0.33$, medium for $0.33 \le |d| < 0.474$, and large for $|d| \ge 0.474$ \cite{Gris2005a}.

On the qualitative side, we manually inspected 300 of the partially correct predictions generated by both techniques and having all information but the log message correctly predicted. The goal of the inspection was to verify whether the generated log message, while different from the target one, was semantically equivalent to it. To this aim, two of the authors independently inspected all 600 log messages (300 for each approach), with $\sim$11\% (70) arisen conflicts being solved by a third author. We report the percentage of ``wrong'' log messages generated by both techniques classified as semantically equivalent to the target one.

To answer RQ$_2$ and evaluate the extent to which \approach is able to correctly inject multiple log statements, we run \approach against the test set reported in \tabref{tab:ds-summary-1} (see row \emph{Fine-tuning: Multi-log Injection with IR}). We then report the percentage of correct predictions generated by the approach (\ie methods for which all $n$ log statements that \approach was supposed to generate and inject have been correctly predicted). In this case we do not compute the partially correct predictions since, if a prediction is not completely correct, it is not possible to match the generated log statements with the target ones to compare them. To make this concept more clear, consider the case in which \approach was asked to generate two log statements $s_1$ and $s_2$ but it only injects one statement $s_i$, being different from both $s_1$ and $s_2$. We cannot know whether $s_i$ should be compared with $s_1$ or with $s_2$ to assess the percentage of partially correct predictions in terms of \eg log level. For this reason, we only focus on the predictions being 100\% correct (\ie the output method is identical to the target one). 

To answer RQ$_3$, we run \approach against the test sets presented in \tabref{tab:ds-summary-2}, reporting the confusion matrix of the generated predictions and the corresponding accuracy, recall, and precision. We compare these results with those of: (i) an \emph{optimistic} classifier always predicting \emph{true} (\ie the method is in need for log statements); (ii) a \emph{pessimistic} classifier always predicting \emph{false} (\ie no need for log statements); and (iii) a random classifier, randomly predicting \emph{true} or \emph{false} for each input instance. We use the same statistical analysis described for RQ$_1$ to compare \approach with the baselines.